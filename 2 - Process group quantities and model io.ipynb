{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "home = os.path.expanduser(\"~\")\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import pickle\n",
    "import gc\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=RuntimeWarning)\n",
    "\n",
    "import datafunctions as dfunc\n",
    "import headingchange_models as hm\n",
    "dt=1/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## data and file location definitions\n",
    "datadir = '/media/jacob/JD_DATA/zfish/'\n",
    "resultdir = 'savedresults/'  \n",
    "\n",
    "## note that 'treatment' refers to different lines.  would be more accurate to label as 'line' instead of 'treatment', e.g. focuslines instead of focustreatments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook contains things to run once and save output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Step 0):  simple quantities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run and save some simple treatment quantities (just run once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numtreatments: 91\n",
      "[ 0 46 65  4 50  2]\n",
      "[ 1 48 56 57 58 59 60 49 61 62 63 64 51 66 52 53 54 41 55 67 68 42 69 70\n",
      " 71 72 73 74 75 43 76 44 77 78 45 79 80  3 81 47 82 83 84 34 85 86 87 88\n",
      " 89 90 35 13 14 15 16 17 36 18 19  5 20  6 37  7 21  8 26  9 22 38 39 23\n",
      " 40 24 25 10 27 28 29 30 11 31 32 12 33]\n"
     ]
    }
   ],
   "source": [
    "# datadir = '/mnt/storage/zfish/'\n",
    "# resultdir = home+'/Dropbox/zfish/savedresults/'\n",
    "treatments = [str.split(s,'/')[-2] for s in sorted(glob.glob(datadir+\"/*/*\"+ os.path.sep))]\n",
    "treatments.remove('chd8KO_WT')  # remove this because the groups were half mutants and half WT fish\n",
    "treatments.remove('oxtHO_verified')  # remove because unsure what this mutation was (documentation lost)\n",
    "\n",
    "pickle.dump([treatments],open(resultdir+'treatmentlist.pkl','wb'))\n",
    "\n",
    "numtreatments = len(treatments)\n",
    "print('numtreatments:',numtreatments)\n",
    "\n",
    "focustreatments=np.ndarray.flatten(np.array([np.where([t==ft for t in treatments]) for ft in \n",
    "               ['WT','immp2lHO','ctnnd2bHO','scn1labHT','disc1KO','chrna2aHO']]))                                          \n",
    "sel = np.tile(True,numtreatments)\n",
    "sel[focustreatments] = False\n",
    "notfocus = np.arange(numtreatments)[sel]\n",
    "notfocus = notfocus[np.argsort(np.array(treatments)[notfocus])]\n",
    "\n",
    "print(focustreatments)\n",
    "print(notfocus)\n",
    "\n",
    "pickle.dump([focustreatments,notfocus],open(resultdir+'focustreatmentlist.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Run to get all number of trials (just run this once and save results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_numtrials(datadir,treatment):  # a terrible way to get the number of trials, so just run it once and save results\n",
    "    [_,_,_,_,\n",
    "                    _,_,_,\n",
    "                    _,_,_,\n",
    "                    datafiles,_] = pickle.load(open(datadir+treatment+'-alltrials.pkl','rb'))\n",
    "\n",
    "    numtrials = len(datafiles)\n",
    "    return  numtrials\n",
    "\n",
    "allnumtrials = np.array([get_numtrials(datadir,t) for t in treatments])\n",
    "pickle.dump([allnumtrials],open(resultdir+'allnumtrials.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (step 0) Read in treatment quantities, if already ran the above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['WT', 'adra1aaHO', 'chrna2aHO', 'kctd13KO', 'scn1labHT',\n",
       "       'shank3bKO', 'slc18a2HT', 'slc22a15HO', 'slc25a27HO', 'slc39a11HO',\n",
       "       'srrHO', 'tph2HO', 'ube3aKO', 'pard3baHO', 'pcdh10bHO', 'pdfHO',\n",
       "       'pomcbHO', 'prkg1aHO', 'setd8aHO', 'shank3aHO', 'slc16a3HO',\n",
       "       'slc25a14HO', 'slc4a10bHO', 'slc6a7HO', 'slc9a6aHO', 'slc9a6bHO',\n",
       "       'slc30a5HO', 'sst1,1HO', 'sst3HO', 'sstr2bHO', 'stat6HO', 'trhHO',\n",
       "       'trhraHO', 'uts2aHO', 'nfkb1HO', 'oxtKO', 'sapap2KO', 'slc1a1HO',\n",
       "       'slc6a3KO', 'slc6a4aHO', 'slc6a8HO', 'drd4-rsHO', 'esr2aHO',\n",
       "       'gnrhr4HO', 'grm5aHO', 'homer1bHO', 'immp2lHO', 'lrrn3KO',\n",
       "       'adra1abHO', 'chrm4aHO', 'disc1KO', 'dlg4aHO', 'drd1bHO',\n",
       "       'drd2bKO', 'drd3HO', 'drd4aHO', 'adrb3aHO', 'avpHO', 'avpr1abHO',\n",
       "       'ca9HO', 'cdnfHO', 'cnn1bHO', 'cpdbHO', 'csmd1aHO', 'ctnnd2aHO',\n",
       "       'ctnnd2bHO', 'drd1aHO', 'drd6bHO', 'ercc6HO', 'esrraHO',\n",
       "       'esrrgaHO', 'fgf12aHO', 'fgf12bHO', 'gabrpHO', 'galnHO', 'gnrh3HO',\n",
       "       'gpc6aHO', 'hdac5HO', 'hdac9bHO', 'hrh3HO', 'htr1abHO', 'kmt2cbHO',\n",
       "       'moxd1HO', 'ncoa1HO', 'ncor2HO', 'nfkb2HO', 'npas3aHO', 'npy2rHO',\n",
       "       'nr3c1HO', 'nrxn1bHO', 'ntsHO'], dtype='<U10')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[treatments] = pickle.load(open(resultdir+'treatmentlist.pkl','rb'))\n",
    "numtreatments = len(treatments)\n",
    "[focustreatments,notfocus] = pickle.load(open(resultdir+'focustreatmentlist.pkl','rb'))\n",
    "[allnumtrials] = pickle.load(open(resultdir+'allnumtrials.pkl','rb'))\n",
    "focustreatments\n",
    "pxpercm = 4.02361434 * 10  # from tracker\n",
    "np.array(treatments)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (step 1) Calculate distance and rotated neighbor coordinates and save for each line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for treatment in treatments:\n",
    "    print(treatment)\n",
    "\n",
    "    [trial_speeds,trial_trajectories,trial_headings,trial_theta,\n",
    "                    trial_smoothspeeds,trial_smoothtrajectories,trial_smoothheadings,\n",
    "                    trial_ellipses,trial_arena,trial_sex,\n",
    "                    datafiles,trial_trackingerrors] = pickle.load(open(datadir+treatment+'-alltrials.pkl','rb'))\n",
    "    \n",
    "    numtrials = len(datafiles)\n",
    "    numfish = trial_speeds[0].shape[1]\n",
    "\n",
    "    # transpose theta, because the other quantities are stored that way, oops:\n",
    "    trial_theta = [t.T for t in trial_theta]\n",
    "\n",
    "    print(treatment,',', numtrials, 'trials')    \n",
    "    \n",
    "    ddfilename = datadir+treatment+'-dcoords+dist-heading.pkl'        \n",
    "    trial_dcoords = []\n",
    "    trial_dist = []\n",
    "    for trial in range(numtrials):\n",
    "        print(trial,numtrials)\n",
    "        # Rotate coordinates, and make a Katz density plot\n",
    "        #  NOTE THAT THIS IS 'TRAJECTORIES', NOT SMOOTHED\n",
    "        trajectories=trial_trajectories[trial]\n",
    "        theta=trial_smoothheadings[trial]\n",
    "        numsteps=trajectories.shape[0]\n",
    "        alldcoords_rotated = np.zeros((numsteps,numfish,numfish,2))\n",
    "        alldist = np.zeros((numsteps,numfish,numfish)) \n",
    "\n",
    "        for i in range(numfish):\n",
    "            dcoords = np.zeros(trajectories.shape)    \n",
    "            dcoords_rotated = np.zeros(trajectories.shape)    \n",
    "            for step in range(numsteps):\n",
    "                xrot = np.cos(-theta[step,i])\n",
    "                yrot = np.sin(-theta[step,i])\n",
    "                dcoords[step] = trajectories[step] - trajectories[step,i]\n",
    "                dcoords_rotated[step] = np.dot([[xrot,-yrot],[yrot,xrot]],dcoords[step].T).T\n",
    "            dist =np.sqrt(dcoords_rotated[:,:,0]**2+dcoords_rotated[:,:,1]**2)\n",
    "            alldist[:,i] = dist\n",
    "            alldcoords_rotated[:,i] = dcoords_rotated\n",
    "\n",
    "        trial_dcoords.append(alldcoords_rotated)\n",
    "        trial_dist.append(alldist)\n",
    "\n",
    "    # output coordinate transformed neighbors to file\n",
    "    pickle.dump([trial_dcoords,trial_dist],open(ddfilename,'wb'))\n",
    "    print('wrote dcoords to:', ddfilename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (step 2) Quantiles, boundary, group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnumbins = 20  # number of distance bins\n",
    "\n",
    "alldskip=50 # for saving these, which are used all together to get 'combined quantiles'\n",
    "\n",
    "\n",
    "for tnum in range(numtreatments):\n",
    "    gc.collect()\n",
    "    treatment = treatments[tnum]\n",
    "\n",
    "    ### TREATMENT\n",
    "    print(tnum, treatment)\n",
    "    numtrials=allnumtrials[tnum]\n",
    "\n",
    "    trialsel='all'\n",
    "    alldist, alldcoords_rotated, smoothtrajectories, smoothspeeds, theta, heading, trajectories, trialids, trackingerrors = dfunc.getcat(treatment,datadir,trialsel)  # do this, because it saves a lot on memory    \n",
    "    \n",
    "    ## Calculate arena border, using function\n",
    "    trial_arena_mid, trial_arena_r = dfunc.getboundaries(trajectories,trialids,numtrials)    \n",
    "    \n",
    "    ## Identify 'tracking errors', using the thresholds:\n",
    "    neighbordist_threshold = 40  # this is about 1 cm\n",
    "    numfish=6\n",
    "    neighborRH_threshold = 0.2 # angular threshold in radians\n",
    "    neighborsel = lambda x: np.reshape(x[:,np.logical_not(np.diag(np.ones(numfish).astype(int)))],(-1,numfish,numfish-1))    \n",
    "    relativeheading = np.array([heading - np.tile(heading[:,i],(numfish,1)).T for i in range(numfish)]).swapaxes(0,1)\n",
    "    overlap_individuals = np.sum(neighborsel((np.cos(relativeheading)>np.cos(neighborRH_threshold)) & (alldist<neighbordist_threshold)),axis=-1)\n",
    "    # now count error time steps as ones where either individuals overlap, or there is a tracking error\n",
    "    errortimesteps = (np.sum(overlap_individuals,axis=-1)>0) | (np.sum(trackingerrors,axis=-1)>0)\n",
    "    \n",
    "    # fractions not used\n",
    "    numsteps = overlap_individuals.shape[0]\n",
    "    removed_te = np.sum(np.sum(trackingerrors,axis=-1)>0)/numsteps\n",
    "    removed_overlap = np.sum( (np.sum(overlap_individuals,axis=-1)>0) & np.logical_not(np.sum(trackingerrors,axis=-1)>0))/numsteps    \n",
    "    indiv_te = np.sum(trackingerrors)/(numsteps*numfish)\n",
    "    indiv_overlap = np.sum(overlap_individuals & np.logical_not(trackingerrors))/(numsteps*numfish)\n",
    "\n",
    "    ## Calculate group heading, rotation and polarization, then the rotated coordinates\n",
    "    groupcentroid = np.mean(trajectories,axis=1)\n",
    "    gcdiff=np.gradient(groupcentroid,axis=0)/dt\n",
    "    groupspeed=np.sqrt(gcdiff[:,0]**2+gcdiff[:,1]**2)\n",
    "\n",
    "    groupheadingxy=gcdiff/(groupspeed[:, np.newaxis])\n",
    "    groupheading=np.arctan2(groupheadingxy[:,1],groupheadingxy[:,0])\n",
    "    positions_relative=np.swapaxes(np.array([trajectories[:,k]-groupcentroid for k in range(numfish)]),0,1)\n",
    "    # calculate the polarization and rotation coefficients, using the equations from Tunstrom 2013\n",
    "    grouppolarization=np.sqrt(np.nanmean(np.cos(heading),axis=1)**2 + np.nanmean(np.sin(heading),axis=1)**2)\n",
    "    xall = positions_relative/np.expand_dims(np.linalg.norm(positions_relative,axis=2),2)\n",
    "    yall = np.swapaxes(([np.cos(heading),np.sin(heading)]),1,2).T\n",
    "    grouprotation = np.mean(np.cross(xall,yall),axis=1)\n",
    "\n",
    "    mask = np.ones((numfish,numfish), dtype=bool)\n",
    "    np.fill_diagonal(mask,0)\n",
    "    groupiid = np.mean(alldist[:,mask],axis=1)          \n",
    "            \n",
    "    ## Calculate boundary distances\n",
    "    fn = (lambda x: np.sqrt(x[:,:,0]**2+x[:,:,1]**2))\n",
    "    boundarydist = np.concatenate([r-fn(trajectories[trialids==n]-mid) \n",
    "                         for n, r, mid in zip(range(numtrials),trial_arena_r,trial_arena_mid)])\n",
    "\n",
    "    #### QUANTILE CALCULATION\n",
    "    # speed quantiles\n",
    "    quantilefilter = np.tile(errortimesteps,(numfish,1)).T  # don't include error time steps (from overlap or tracking errors), or boundary id error cases\n",
    "    scat = smoothspeeds[quantilefilter]\n",
    "    speedquantiles10 = np.quantile(scat,np.linspace(0,1,11))\n",
    "    speedquantiles20 = np.quantile(scat,np.linspace(0,1,21))   \n",
    "\n",
    "    # neighbor distance quantiles\n",
    "    sel_offdiag = np.logical_not(np.diag(np.ones(numfish).astype(int)))\n",
    "    ncat = np.reshape(alldist[:,sel_offdiag],(-1,6,5))\n",
    "    # for simplicity, only filter probabilities by the focal fish\n",
    "    ncat = np.reshape(ncat[quantilefilter],(-1))\n",
    "    rmin = 0  # for neighbor, use a min\n",
    "    ndistquantiles20 = np.quantile(ncat[ncat>rmin],np.linspace(0,1,rnumbins+1))\n",
    "\n",
    "    # boundary distance quantiles\n",
    "    brmin = 0  # this can be negative if the boundary is identified wrong, so use a threshold\n",
    "    bcat = np.reshape(boundarydist[quantilefilter],(-1))\n",
    "    bdistquantiles20 = np.quantile(bcat[bcat>brmin],np.linspace(0,1,rnumbins+1))\n",
    "\n",
    "    # save these results\n",
    "    outfile = datadir+treatment+'-quantile+group+boundary.pkl'\n",
    "    pickle.dump([speedquantiles10, speedquantiles20, ndistquantiles20, bdistquantiles20,\n",
    "        groupcentroid, groupspeed, grouppolarization, grouprotation, groupiid, boundarydist,\n",
    "        trial_arena_mid, trial_arena_r,errortimesteps]\n",
    "                ,open(outfile,'wb') )\n",
    "    # save things for calculating the combined distributions\n",
    "    outfile = datadir+treatment+'quantile-short.pkl'\n",
    "    pickle.dump([scat[::alldskip],ncat[::alldskip],bcat[::alldskip], removed_te, removed_overlap, indiv_te, indiv_overlap],open(outfile,'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrote to: /media/jacob/JD_DATA/zfish-code+save/savedresults/combinedquantiles.pkl\n"
     ]
    }
   ],
   "source": [
    "###  get the combined distributions for speed, ndist, bdist\n",
    "grid_scat = dfunc.initarray([numtreatments])\n",
    "grid_ncat = dfunc.initarray([numtreatments])\n",
    "grid_bcat = dfunc.initarray([numtreatments])\n",
    "\n",
    "grid_removed_because_of_errors = np.zeros((numtreatments,2))\n",
    "grid_individual_errors = np.zeros((numtreatments,2))\n",
    "\n",
    "for tnum in range(numtreatments):\n",
    "    treatment = treatments[tnum]\n",
    "    outfile = datadir+treatment+'quantile-short.pkl'\n",
    "    if os.path.isfile(outfile):\n",
    "        scat, ncat, bcat, removed_te, removed_overlap, indiv_te, indiv_overlap = pickle.load(open(outfile,'rb'))\n",
    "        grid_scat[tnum] = scat\n",
    "        grid_ncat[tnum] = ncat\n",
    "        grid_bcat[tnum] = bcat\n",
    "        grid_removed_because_of_errors[tnum] = (removed_te,removed_overlap)\n",
    "        grid_individual_errors[tnum] = (indiv_te,indiv_overlap)    \n",
    "    else:\n",
    "        print(tnum,':  no file')\n",
    "\n",
    "rnumbins = 20  # number of distance bins\n",
    "all_speedquantiles10 = np.quantile(np.concatenate(grid_scat),np.linspace(0,1,11))\n",
    "all_speedquantiles20 = np.quantile(np.concatenate(grid_scat),np.linspace(0,1,21))\n",
    "temp = np.concatenate(grid_ncat)\n",
    "all_ndistquantiles20 = np.quantile(temp[temp>0],np.linspace(0,1,rnumbins+1))\n",
    "temp = np.concatenate(grid_bcat)\n",
    "all_bdistquantiles20 = np.quantile(temp[temp>0],np.linspace(0,1,rnumbins+1))\n",
    "\n",
    "\n",
    "outfile = resultdir+'combinedquantiles.pkl'\n",
    "pickle.dump([all_speedquantiles10, all_speedquantiles20, all_ndistquantiles20, all_bdistquantiles20]\n",
    "            ,open(outfile,'wb') )\n",
    "print('wrote to:',outfile)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (step 3) Loop through to get all medians and save to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datachoice = 1  # 0 = all, 1= no overlap, 2 = far, 3=no tracking errors\n",
    "\n",
    "\n",
    "def varcoeff(data):\n",
    "    return np.median(np.std(data,axis=1)/np.std(data))\n",
    "\n",
    "def varcoeff_IQR(data):\n",
    "    ingroup = np.median( np.std(data,axis=1)     )\n",
    "    total = np.quantile(data,0.75)-np.quantile(data,0.25)\n",
    "    return ingroup\n",
    "\n",
    "[trial_speedmedians, trial_speedIQR, trial_nnmedians, trial_cosmedians, trial_groupnums, trial_groupiidmedians, trial_grouppolmedians, \n",
    " trial_groupiid_together_medians, trial_grouppol_together_medians,\n",
    " trial_freezefrac, trial_freezefrac1sec, trial_speedsync, trial_centroiddist, trial_boundarydist, trial_nnrelspeed\n",
    "] = np.empty((15,len(treatments)),dtype=list)\n",
    "\n",
    "[treatment_speedmedians, treatment_speedIQR, treatment_nnmedians, treatment_cosmedians, treatment_groupnums, treatment_groupiidmedians, treatment_grouppolmedians, \n",
    " treatment_groupiid_together_medians, treatment_grouppol_together_medians,\n",
    " treatment_freezefrac, treatment_freezefrac1sec, treatment_speedsync, treatment_centroiddist, treatment_boundarydist, treatment_nnrelspeed\n",
    "] = np.zeros((15,len(treatments)))\n",
    "\n",
    "for tnum in range(numtreatments):\n",
    "    print(tnum)\n",
    "    treatment = treatments[tnum]\n",
    "    numtrials = allnumtrials[tnum]\n",
    "    alldist, alldcoords_rotated, smoothtrajectories, smoothspeeds, theta, heading, trajectories, trialids, trackingerrors = dfunc.getcat(treatment,datadir,'all')  # do this, because it saves a lot on memory  \n",
    "    numsteps = alldist.shape[0]\n",
    "    numfish = alldist.shape[1]\n",
    "\n",
    "    outfile = datadir+treatment+'-quantile+group+boundary.pkl'\n",
    "    [speedquantiles10, speedquantiles20, ndistquantiles, bdistquantiles,\n",
    "        groupcentroid, groupspeed, grouppolarization, grouprotation, groupiid, boundarydist,\n",
    "        trial_arena_mid, trial_arena_r, errortimesteps    ] = pickle.load(open(outfile,'rb'))\n",
    "\n",
    "\n",
    "    # nearest neighbor\n",
    "    neighborsel = lambda x: np.reshape(x[:,np.logical_not(np.diag(np.ones(numfish).astype(int)))],(-1,6,5))\n",
    "    nnums = np.argsort(np.argsort(neighborsel(alldist),axis=2),axis=2)\n",
    "    n=0  # nearest neighbor is n=0\n",
    "    nndata = np.reshape(neighborsel(alldist)[nnums==n],(numsteps,-1))  \n",
    "    nnheading =  np.reshape(neighborsel(np.tile(heading,(numfish,1,1)).swapaxes(0,1))[nnums==n],(numsteps,-1))  \n",
    "\n",
    "    relativespeeds = smoothspeeds[:,np.newaxis,:] - smoothspeeds[:,:,np.newaxis]\n",
    "    nnrelspd = np.reshape(neighborsel(relativespeeds)[nnums==n],(numsteps,-1))\n",
    "\n",
    "    cosalign = np.abs(np.cos((heading[:,:,np.newaxis] - heading[:,np.newaxis,:]))    )\n",
    "    meancosalign = np.mean(neighborsel(cosalign),axis=2)\n",
    "\n",
    "    # group membership\n",
    "    outfile = datadir+treatment+'-groupmembership.pkl'\n",
    "    groupmembership,groupnumber,groupnumcomponents,cutoff = pickle.load(open(outfile,'rb'))\n",
    "    grouptogether = np.all(groupmembership==0,axis=1)\n",
    "\n",
    "    # freezing\n",
    "    def getfreezesel(freezemedian):\n",
    "        rs, frac = dfunc.mmed_all(smoothspeeds,freezemedian,skipcalc=1)\n",
    "        freezesel = (rs<all_speedquantiles20[1]) & np.logical_not(np.isnan(rs))       \n",
    "        return freezesel\n",
    "    freezesel600 = getfreezesel(600)\n",
    "    freezesel60 = getfreezesel(60)\n",
    "\n",
    "    #filters\n",
    "    far10 = boundarydist > 0.1*np.median(trial_arena_r)\n",
    "\n",
    "    # use this line to select all data or not\n",
    "    # make sure to keep the variable definition of 'far', because it is used below\n",
    "    if (datachoice==0):\n",
    "        datasel = np.tile(True,boundarydist.shape)\n",
    "        speedsel = datasel\n",
    "    elif datachoice==1:\n",
    "        datasel = np.logical_not(np.tile(errortimesteps,(numfish,1)).T)\n",
    "        speedsel = datasel            \n",
    "    elif datachoice==2:\n",
    "        datasel = far10\n",
    "        speedsel = datasel            \n",
    "    elif datachoice==3:\n",
    "        datasel = np.logical_not(np.tile(errortimesteps,(numfish,1)).T) \n",
    "        speedsel = datasel & np.logical_not(freezesel600)\n",
    "    else:\n",
    "        print('error in datachoice number')\n",
    "\n",
    "    dataselcount = np.sum( datasel,axis=1)  \n",
    "    dataseltimesN = np.array([dataselcount>=i+1 for i in range(numfish)])    \n",
    "    nsel_datasel = 3  # if make this 5, then actually have no data for some trials with the boundary threshold.  With the tracking errors, it doesn't matter - these are tiled per frame\n",
    "\n",
    "    datasel_group = dataseltimesN[nsel_datasel]\n",
    "\n",
    "    # initialize\n",
    "    [ speedmedians, speedIQR, speedstd,  nnmedians,  cosmedians,  groupnums,  groupiidmedians, grouppolmedians, \n",
    "        groupiid_together_medians,    grouppol_together_medians,  freezefrac, freezefrac1sec, group_centroiddist, group_speedsync, group_speedsyncvar, \n",
    "        group_boundarydist,  nnrelspeed ]  = np.zeros((17,numtrials+1))\n",
    "\n",
    "    for j in range(numtrials+1):\n",
    "        if j==0:  # then select all\n",
    "            trial=-1\n",
    "            trialsel = np.tile(True,trialids.shape)\n",
    "        else:\n",
    "            trial=j-1\n",
    "            trialsel = trialids==trial\n",
    "        speedmedians[j] = np.median(smoothspeeds[trialsel[:,np.newaxis] & speedsel])\n",
    "        speedIQR[j] = np.diff(np.quantile(smoothspeeds[trialsel[:,np.newaxis] & speedsel],[0.25,0.75]))        \n",
    "        speedstd[j] = np.std(smoothspeeds[trialsel[:,np.newaxis] & speedsel])\n",
    "        nnrelspeed[j] = np.diff(np.quantile(nnrelspd[trialsel[:,np.newaxis] & speedsel],[0.25,0.75]))                    \n",
    "        nnmedians[j] = np.median(nndata[trialsel[:,np.newaxis] & datasel])\n",
    "        cosmedians[j] = np.median(meancosalign[trialsel[:,np.newaxis] & datasel])\n",
    "        groupnums[j] = np.mean(groupnumber[trialsel & datasel_group])     \n",
    "\n",
    "        groupiidmedians[j] = np.nanmedian(groupiid[trialsel & datasel_group])\n",
    "        grouppolmedians[j] = np.nanmedian(grouppolarization[trialsel & datasel_group])        \n",
    "        groupiid_together_medians[j] = np.nanmedian(groupiid[trialsel&grouptogether&datasel_group])\n",
    "        grouppol_together_medians[j] = np.nanmedian(grouppolarization[trialsel&grouptogether&datasel_group])   \n",
    "\n",
    "        totalcounts = np.sum(trialsel[:,np.newaxis] & datasel)\n",
    "        freezefrac[j] = np.sum(freezesel600[trialsel[:,np.newaxis] & datasel]) / totalcounts\n",
    "        freezefrac1sec[j] = np.sum(freezesel60[trialsel[:,np.newaxis] & datasel]) / totalcounts        \n",
    "\n",
    "        dc = np.diff(groupcentroid[trialsel],axis=0)\n",
    "        dcdist = np.sqrt(dc[:,0]**2+dc[:,1]**2)        \n",
    "        group_centroiddist[j] = np.nanmedian(dcdist[datasel_group[trialsel][0:-1] & datasel_group[trialsel][1:]])\n",
    "        group_speedsync[j] = varcoeff_IQR(smoothspeeds[trialsel & datasel_group])\n",
    "        group_speedsyncvar[j] = varcoeff(smoothspeeds[trialsel & datasel_group])\n",
    "        group_boundarydist[j] = np.nanmedian(boundarydist[trialsel])\n",
    "\n",
    "\n",
    "    treatment_speedmedians[tnum], trial_speedmedians[tnum] = speedmedians[0], speedmedians[1:]\n",
    "    treatment_speedIQR[tnum], trial_speedIQR[tnum] = speedIQR[0], speedIQR[1:]\n",
    "    treatment_nnmedians[tnum], trial_nnmedians[tnum] = nnmedians[0], nnmedians[1:]\n",
    "    treatment_cosmedians[tnum], trial_cosmedians[tnum] = cosmedians[0], cosmedians[1:]\n",
    "\n",
    "    treatment_groupnums[tnum], trial_groupnums[tnum] = groupnums[0], groupnums[1:]\n",
    "    treatment_groupiidmedians[tnum], trial_groupiidmedians[tnum] = groupiidmedians[0], groupiidmedians[1:]\n",
    "    treatment_grouppolmedians[tnum], trial_grouppolmedians[tnum] = grouppolmedians[0], grouppolmedians[1:]\n",
    "    treatment_groupiid_together_medians[tnum], trial_groupiid_together_medians[tnum] = groupiid_together_medians[0], groupiid_together_medians[1:]\n",
    "    treatment_grouppol_together_medians[tnum], trial_grouppol_together_medians[tnum] = grouppol_together_medians[0], grouppol_together_medians[1:]\n",
    "\n",
    "    treatment_freezefrac[tnum], trial_freezefrac[tnum] = freezefrac[0], freezefrac[1:]\n",
    "    treatment_freezefrac1sec[tnum], trial_freezefrac1sec[tnum] = freezefrac1sec[0], freezefrac1sec[1:]    \n",
    "    treatment_centroiddist[tnum], trial_centroiddist[tnum] = group_centroiddist[0], group_centroiddist[1:]\n",
    "    treatment_speedsync[tnum], trial_speedsync[tnum] = group_speedsync[0], group_speedsync[1:]\n",
    "    treatment_boundarydist[tnum], trial_boundarydist[tnum] = group_boundarydist[0], group_boundarydist[1:]\n",
    "\n",
    "    treatment_nnrelspeed[tnum], trial_nnrelspeed[tnum] = nnrelspeed[0], nnrelspeed[1:]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### TRIAL save results to pickle file\n",
    "\n",
    "# use this line to select all data or not\n",
    "if (datachoice==0):\n",
    "    postfix=''\n",
    "elif datachoice==1:\n",
    "    postfix='-nooverlap'\n",
    "elif datachoice==2:\n",
    "    postfix='-far'\n",
    "elif datachoice==3:\n",
    "    postfix='nooverlap-nofreezeforspeed'\n",
    "else:\n",
    "    print('error in datachoice number')    \n",
    "outfile = resultdir + 'Fig2-TrialQuantities'+postfix+'.pkl'\n",
    "pickle.dump([\n",
    "trial_speedmedians, trial_speedIQR, trial_nnmedians, trial_cosmedians, trial_groupnums, trial_groupiidmedians, trial_grouppolmedians, \n",
    " trial_groupiid_together_medians, trial_grouppol_together_medians,\n",
    " trial_freezefrac, trial_freezefrac1sec, trial_speedsync, trial_centroiddist, trial_boundarydist, trial_nnrelspeed\n",
    "],open(outfile,'wb'))\n",
    "\n",
    "### TREATMENT save results to pickle file\n",
    "outfile = resultdir + 'Fig2-TreatmentQuantities'+postfix+'.pkl'\n",
    "pickle.dump([\n",
    "treatment_speedmedians, treatment_speedIQR, treatment_nnmedians, treatment_cosmedians, treatment_groupnums, treatment_groupiidmedians, treatment_grouppolmedians, \n",
    " treatment_groupiid_together_medians, treatment_grouppol_together_medians,\n",
    " treatment_freezefrac, treatment_freezefrac1sec, treatment_speedsync, treatment_centroiddist, treatment_boundarydist, treatment_nnrelspeed\n",
    "],open(outfile,'wb'))\n",
    "print('wrote results to file')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "#  (step 4) Make input-outputs for model fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# immport combined quantiles\n",
    "outfile = resultdir+'combinedquantiles.pkl'\n",
    "[all_speedquantiles10, all_speedquantiles20, all_ndistquantiles20, all_bdistquantiles20] = pickle.load(open(outfile,'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions related to defining 'freezing', so that can filter results\n",
    "def movingaverage(data,N):\n",
    "    # Pandas syntax is ridiculous to me, but this indeed works.  see https://stackoverflow.com/questions/13728392/moving-average-or-running-mean/30141358#30141358\n",
    "    return pd.Series(data).rolling(window=N).mean().iloc[N-1:].values\n",
    "def movingmedian(data,N):\n",
    "    # Pandas syntax is ridiculous to me, but this indeed works.  see https://stackoverflow.com/questions/13728392/moving-average-or-running-mean/30141358#30141358\n",
    "#     return pd.Series(data).rolling(window=N).median().iloc[N-1:].values\n",
    "    return pd.Series(data).rolling(window=N,center=True).median().values\n",
    "\n",
    "def mmed_all(smoothspeeds,N,skipcalc=5,threshold=all_speedquantiles20[1]):\n",
    "    skipcalc = 1 if N==1 else skipcalc        \n",
    "    allmed = np.array([movingmedian(smoothspeeds[::skipcalc,i],int(N/skipcalc))for i in range(6)]).T\n",
    "    return allmed, np.mean(allmed<threshold)\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "def getcomponents(Aij_single):\n",
    "    G = nx.from_numpy_matrix(Aij_single)\n",
    "    connected = list(nx.connected_components(G))\n",
    "    componentsizes =[len(c) for c in connected]\n",
    "    components = np.zeros(numfish, dtype=int) - 1\n",
    "    for i in range(len(connected)):\n",
    "        components[list(connected[i])]=i    \n",
    "    largestgroupsize = np.max(componentsizes)\n",
    "    numcomponents = len(componentsizes)\n",
    "    return components, largestgroupsize, numcomponents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trial quantities\n",
    "postfix_s = '-nooverlap'\n",
    "outfile = resultdir + 'Fig2-TrialQuantities'+postfix_s+'.pkl'\n",
    "trial_speedmedians_all = pickle.load(open(outfile,'rb'))[0]\n",
    "\n",
    "medmedspeed = np.array([np.median(t) for t in trial_speedmedians_all] )  # use this to adjust the delay time\n",
    "\n",
    "numfish=6\n",
    "\n",
    "# make model io (done here only for focustreatments)\n",
    "for tnum in focustreatments:\n",
    "    treatment = treatments[tnum]\n",
    "\n",
    "    ### TREATMENT\n",
    "    print(tnum, treatment)\n",
    "    numtrials=allnumtrials[tnum]\n",
    "    trialsel='all'\n",
    "    alldist, alldcoords_rotated, smoothtrajectories, smoothspeeds, theta, heading, trajectories, trialids,trackingerrors = dfunc.getcat(treatment,datadir,trialsel)  # do this, because it saves a lot on memory    \n",
    "    numsteps = alldist.shape[0]\n",
    "    \n",
    "    # Group membership - get which are in the largest group.  (not using this metric anymore)\n",
    "#     groupmembership = np.zeros(smoothspeeds.shape)\n",
    "#     groupnumber = np.zeros(smoothspeeds.shape[0])\n",
    "#     groupnumcomponents = np.zeros(smoothspeeds.shape[0])\n",
    "#     cutoff = 286.86\n",
    "#     for step in range(numsteps):\n",
    "#         Aij = np.heaviside(cutoff-alldist[step],0) \n",
    "#         Aij[range(numfish),range(numfish)] = 0  \n",
    "#         groupmembership[step], groupnumber[step], groupnumcomponents[step]  = getcomponents(Aij)\n",
    "    outfile = datadir+treatment+'-groupmembership.pkl'\n",
    "    groupmembership,groupnumber,groupnumcomponents,cutoff = pickle.load(open(outfile,'rb'))\n",
    "    print('read in data from:',outfile)    \n",
    "#     pickle.dump([groupmembership,groupnumber,groupnumcomponents,cutoff],open(outfile,'wb'))\n",
    "#     print('wrote to:',outfile)\n",
    "    \n",
    "    outfile = datadir+treatment+'-quantile+group+boundary.pkl'\n",
    "    [speedquantiles10, speedquantiles20, ndistquantiles20, bdistquantiles20,    _, _, _, _, _, boundarydist,\n",
    "            trial_arena_mid, trial_arena_r,errortimesteps] = pickle.load(open(outfile,'rb'))\n",
    "    \n",
    "\n",
    "    # Calculate boundary coordinates, as if the boundary was a \"neighbor\"\n",
    "    allbcoords = np.zeros(trajectories.shape)\n",
    "    allbcoords_rotated = np.zeros(trajectories.shape)\n",
    "    for i in range(numfish):\n",
    "        thetafish = np.arctan2(trajectories[:,i,1]-trial_arena_mid[trialids,1],trajectories[:,i,0]-trial_arena_mid[trialids,0])  \n",
    "        eboundary = np.tile(trial_arena_r[trialids],(2,1)).T * np.array([np.cos(thetafish),np.sin(thetafish)]).T # vector to the boundary\n",
    "        allbcoords[:,i] = eboundary - (trajectories[:,i]-trial_arena_mid[trialids])    \n",
    "        rel_orientation = thetafish-heading[:,i]\n",
    "        x = boundarydist[:,i] * np.cos(rel_orientation)\n",
    "        y = boundarydist[:,i] * np.sin(rel_orientation)    \n",
    "        allbcoords_rotated[:,i] = np.array([x,y]).T    \n",
    "\n",
    "    del allbcoords\n",
    "    gc.collect()\n",
    "\n",
    "    # freezing\n",
    "    medianavgspeeds600frames, frac = dfunc.mmed_all(smoothspeeds,600,skipcalc=1)     \n",
    "    medianavgspeeds600frames[np.isnan(medianavgspeeds600frames)] = 0 # these will be in the 'negative' bin, so will ignore in getinputoutputs fn.  This is only at the start and end of the array    \n",
    "    medianavgspeeds60frames, frac = dfunc.mmed_all(smoothspeeds,60,skipcalc=1)     \n",
    "    medianavgspeeds60frames[np.isnan(medianavgspeeds60frames)] = 0 # these will be in the 'negative' bin, so will ignore in getinputoutputs fn.  This is only at the start and end of the array        \n",
    "    \n",
    "    # Save the input data\n",
    "    inputdata = [smoothspeeds,heading,boundarydist,groupmembership,alldist,alldcoords_rotated,allbcoords_rotated,medianavgspeeds600frames,medianavgspeeds60frames]    \n",
    "    pickle.dump(inputdata,open(datadir+treatment+'-inputdata.pkl','wb'))\n",
    "    \n",
    "    ######################################### BINNING AND SAVING IO #########################################\n",
    "    # define bins to use for encoding\n",
    "    thetanumbins = 32\n",
    "    offsettheta = False  # if true, then will rotate bins to get \"front\", \"back\", \"side\" centers\n",
    "    dtheta = 2*np.pi/(thetanumbins)\n",
    "    theta_edge = np.linspace(-np.pi,np.pi,thetanumbins+1) - offsettheta*dtheta/2\n",
    "    \n",
    "\n",
    "    binscheme='A10'\n",
    "    if binscheme=='A': # 20 bins for all except neighbor speed\n",
    "        sq, sjq, nq, bq = all_speedquantiles20, all_speedquantiles10, all_ndistquantiles20, all_bdistquantiles20\n",
    "    elif binscheme=='A10':  # 10 bins for all\n",
    "        sq, sjq, nq, bq = all_speedquantiles10, all_speedquantiles10, all_ndistquantiles20[::2], all_bdistquantiles20[::2]\n",
    "    elif binscheme=='T':  # use 'treatment' level bins\n",
    "        sq, sjq, nq, bq = speedquantiles20, speedquantiles10, ndistquantiles20, bdistquantiles20\n",
    "    elif binscheme=='T10':  # use 'treatment' level bins\n",
    "        sq, sjq, nq, bq = speedquantiles10, speedquantiles10, ndistquantiles20[::2], bdistquantiles20[::2]\n",
    "\n",
    "    bdist_threshold=0.05\n",
    "    speed_threshold = all_speedquantiles20[1]/2\n",
    "    freeze_threshold = all_speedquantiles20[1]\n",
    "\n",
    "    # input data bins\n",
    "    binnedinputdata = hm.getbins(inputdata,[sq,sjq,nq,bq,theta_edge],\n",
    "                         speed_threshold = speed_threshold, freeze_threshold = freeze_threshold)\n",
    "\n",
    "    # output data (calculate here, not in a function)\n",
    "    shifts_const = np.array([30,60])  # for heading change shifts, to make model output   \n",
    "    shifts_adjusted =  np.round(medmedspeed[0]/medmedspeed[tnum] * shifts_const).astype(int)\n",
    "    shifts = np.concatenate((shifts_const,shifts_adjusted))\n",
    "    \n",
    "    \n",
    "    headingchange_byshift = [dfunc.get_headingchange(data=heading,shift=s) for s in shifts]\n",
    "    vectorheadingchange_byshift = [dfunc.get_vectorheadingchange(data=trajectories,data2=heading,shift=s) for s in shifts]\n",
    "    \n",
    "    outputdata = headingchange_byshift + vectorheadingchange_byshift\n",
    "    numheadingchangesteps = np.min([h.shape[0] for h in outputdata])\n",
    "    binnedinputdata = [bd[0:numheadingchangesteps] for bd in binnedinputdata] \n",
    "    outputdata = [bd[0:numheadingchangesteps] for bd in outputdata] \n",
    "    \n",
    "    # some filters    \n",
    "    # for boundary distance, using the median trial arena_r size to set a threshold, for simplicity\n",
    "    WT_median_trial_arena_r = 920.6505385557605\n",
    "    far = boundarydist > bdist_threshold*WT_median_trial_arena_r\n",
    "    \n",
    "    # for when there are errors in the future timestep\n",
    "    futureerrors = np.tile(False,(numsteps,numfish))\n",
    "    for sh in shifts:\n",
    "        # use individual tracking errors for this, not 'group'.\n",
    "        # because its easier, don't use ones where any of the future cases have errors\n",
    "        futureerrors[0:numsteps-sh] = futureerrors[0:numsteps-sh] | trackingerrors[sh:] \n",
    "    \n",
    "    # this removes cases where the individual is far from the boundary, none are overlapped with other fish \n",
    "    for case in [0]:\n",
    "        if case==0:\n",
    "            farlabel='far05'\n",
    "            sel = far & np.logical_not(errortimesteps[:,np.newaxis])  & np.logical_not(futureerrors)\n",
    "        else:\n",
    "            farlabel= 'all'\n",
    "            sel = np.logical_not(errortimesteps[:,np.newaxis]) & np.logical_not(futureerrors)\n",
    "\n",
    "        inputs, alloutputs, alloutputsraw = hm.getinputoutputs([binnedinputdata,outputdata],sel,tnum,trialids)\n",
    "\n",
    "        gc.collect()\n",
    "        for skip in [1,10,50]:\n",
    "            outfile = datadir+treatment+'-io'+str(skip)+'-'+binscheme+'-'+farlabel+'.pkl'\n",
    "            pickle.dump([inputs[::skip],[o[::skip] for o in alloutputs],[o[::skip] for o in alloutputsraw],sq,nq,bq],open(outfile,'wb'), protocol=4)\n",
    "            print('wrote to',outfile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
